{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Hotness Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc='G:\\\\Projects\\\\Music-Popularity-Predicition\\\\'\n",
    "df=pd.read_csv(loc+'\\\\DataMining\\\\SongsData.csv')\n",
    "df.drop([\"Unnamed: 0\",'id','uri','url','name','explict'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=df.columns.tolist()\n",
    "cols=['duration_ms','tempo','energy', 'danceability','acousticness',\n",
    "      'instrumentalness','liveness','loudness','popularity','speechiness','valence','Hit']\n",
    "df=df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(to_replace =[\"y\", \"n\"],value = [1,0],inplace=True)  \n",
    "#df.replace(to_replace = [\"True\",\"False\"], value=[1,0] , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>tempo</th>\n",
       "      <th>energy</th>\n",
       "      <th>danceability</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>popularity</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>valence</th>\n",
       "      <th>Hit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202066</td>\n",
       "      <td>96.021</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.00112</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.0886</td>\n",
       "      <td>-4.667</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199440</td>\n",
       "      <td>121.998</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.38000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>-4.063</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0554</td>\n",
       "      <td>0.969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>215280</td>\n",
       "      <td>120.042</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.19200</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.0863</td>\n",
       "      <td>-3.497</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>182160</td>\n",
       "      <td>109.891</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.75100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>-5.679</td>\n",
       "      <td>88</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>229573</td>\n",
       "      <td>139.919</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.09060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1470</td>\n",
       "      <td>-2.665</td>\n",
       "      <td>78</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration_ms    tempo  energy  danceability  acousticness  instrumentalness  \\\n",
       "0       202066   96.021   0.698         0.652       0.00112          0.000115   \n",
       "1       199440  121.998   0.813         0.866       0.38000          0.000000   \n",
       "2       215280  120.042   0.762         0.695       0.19200          0.002440   \n",
       "3       182160  109.891   0.405         0.501       0.75100          0.000000   \n",
       "4       229573  139.919   0.800         0.499       0.09060          0.000000   \n",
       "\n",
       "   liveness  loudness  popularity  speechiness  valence  Hit  \n",
       "0    0.0886    -4.667          80       0.0420    0.470    0  \n",
       "1    0.0779    -4.063          84       0.0554    0.969    1  \n",
       "2    0.0863    -3.497          81       0.0395    0.553    0  \n",
       "3    0.1050    -5.679          88       0.0319    0.446    0  \n",
       "4    0.1470    -2.665          78       0.0502    0.272    0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into feature set and target variable\n",
    "X_all = df.drop(['Hit'],1)\n",
    "y_all = df['Hit']\n",
    "\n",
    "# Standardising the data.\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "cols=[['duration_ms','tempo','energy', 'danceability','acousticness',\n",
    "      'instrumentalness','liveness','loudness','popularity','speechiness','valence']]\n",
    "for col in cols:\n",
    "    X_all[col] = scale(X_all[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (11 total features):\n",
      "['duration_ms', 'tempo', 'energy', 'danceability', 'acousticness', 'instrumentalness', 'liveness', 'loudness', 'popularity', 'speechiness', 'valence']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_features(X):\n",
    "    ''' Preprocesses the football data and converts catagorical variables into dummy variables. '''\n",
    "    \n",
    "    # Initialize new output DataFrame\n",
    "    output = pd.DataFrame(index = X.index)\n",
    "\n",
    "    # Investigate each feature column for the data\n",
    "    for col, col_data in X.iteritems():\n",
    "\n",
    "        # If data type is categorical, convert to dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix = col)\n",
    "                    \n",
    "        # Collect the revised columns\n",
    "        output = output.join(col_data)\n",
    "    \n",
    "    return output\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print (\"Processed feature columns ({} total features):\\n{}\".format(len(X_all.columns), list(X_all.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>tempo</th>\n",
       "      <th>energy</th>\n",
       "      <th>danceability</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>popularity</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.452931</td>\n",
       "      <td>-0.925067</td>\n",
       "      <td>-0.045146</td>\n",
       "      <td>0.026772</td>\n",
       "      <td>-0.745531</td>\n",
       "      <td>-0.131310</td>\n",
       "      <td>-0.654203</td>\n",
       "      <td>0.409896</td>\n",
       "      <td>0.886155</td>\n",
       "      <td>-0.529304</td>\n",
       "      <td>-0.244899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.522788</td>\n",
       "      <td>0.076761</td>\n",
       "      <td>0.686743</td>\n",
       "      <td>1.740570</td>\n",
       "      <td>1.122370</td>\n",
       "      <td>-0.132941</td>\n",
       "      <td>-0.738926</td>\n",
       "      <td>0.729857</td>\n",
       "      <td>1.038320</td>\n",
       "      <td>-0.369819</td>\n",
       "      <td>2.073159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101409</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.362166</td>\n",
       "      <td>0.371133</td>\n",
       "      <td>0.195519</td>\n",
       "      <td>-0.098338</td>\n",
       "      <td>-0.672414</td>\n",
       "      <td>1.029687</td>\n",
       "      <td>0.924196</td>\n",
       "      <td>-0.559059</td>\n",
       "      <td>0.140670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.982475</td>\n",
       "      <td>-0.390157</td>\n",
       "      <td>-1.909870</td>\n",
       "      <td>-1.182497</td>\n",
       "      <td>2.951422</td>\n",
       "      <td>-0.132941</td>\n",
       "      <td>-0.524347</td>\n",
       "      <td>-0.126196</td>\n",
       "      <td>1.190484</td>\n",
       "      <td>-0.649513</td>\n",
       "      <td>-0.356388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.278816</td>\n",
       "      <td>0.767901</td>\n",
       "      <td>0.604008</td>\n",
       "      <td>-1.198514</td>\n",
       "      <td>-0.304389</td>\n",
       "      <td>-0.132941</td>\n",
       "      <td>-0.191790</td>\n",
       "      <td>1.470427</td>\n",
       "      <td>0.810073</td>\n",
       "      <td>-0.431709</td>\n",
       "      <td>-1.164689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration_ms     tempo    energy  danceability  acousticness  \\\n",
       "0    -0.452931 -0.925067 -0.045146      0.026772     -0.745531   \n",
       "1    -0.522788  0.076761  0.686743      1.740570      1.122370   \n",
       "2    -0.101409  0.001326  0.362166      0.371133      0.195519   \n",
       "3    -0.982475 -0.390157 -1.909870     -1.182497      2.951422   \n",
       "4     0.278816  0.767901  0.604008     -1.198514     -0.304389   \n",
       "\n",
       "   instrumentalness  liveness  loudness  popularity  speechiness   valence  \n",
       "0         -0.131310 -0.654203  0.409896    0.886155    -0.529304 -0.244899  \n",
       "1         -0.132941 -0.738926  0.729857    1.038320    -0.369819  2.073159  \n",
       "2         -0.098338 -0.672414  1.029687    0.924196    -0.559059  0.140670  \n",
       "3         -0.132941 -0.524347 -0.126196    1.190484    -0.649513 -0.356388  \n",
       "4         -0.132941 -0.191790  1.470427    0.810073    -0.431709 -1.164689  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the feature information by printing the first five rows\n",
    "print (\"\\nFeature values:\")\n",
    "X_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Shuffle and split the dataset into training and testing set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, \n",
    "                                                    random_state = 2,\n",
    "                                                    stratify = y_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Models\n",
    "Because it a classification problem, I have used the following models for predicting Full Time Results :\n",
    "1. Logistic Regression\n",
    "2. Random Foreset\n",
    "3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score (also F-score or F-measure) is a measure of a test's accuracy. \n",
    "#It considers both the precision p and the recall r of the test to compute \n",
    "#the score: p is the number of correct positive results divided by the number of \n",
    "#all positive results, and r is the number of correct positive results divided by \n",
    "#the number of positive results that should have been returned. The F1 score can be \n",
    "#interpreted as a weighted average of the precision and recall, where an F1 score \n",
    "#reaches its best value at 1 and worst at 0.\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    y_pred = clf.predict(features)    \n",
    "    return f1_score(target, y_pred, pos_label=1), sum(target == y_pred) / float(len(y_pred))\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    f1, acc = predict_labels(clf, X_train, y_train)\n",
    "    print (\"F1 score and accuracy score for training set: {:.4f} , {:.4f}.\".format(f1 , acc))\n",
    "    \n",
    "    f1, acc = predict_labels(clf, X_test, y_test)\n",
    "    print (\"F1 score and accuracy score for test set: {:.4f} , {:.4f}.\".format(f1 , acc))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a LogisticRegression using a training set size of 1117. . .\n",
      "F1 score and accuracy score for training set: 0.4617 , 0.6034.\n",
      "F1 score and accuracy score for test set: 0.4478 , 0.6032.\n",
      "\n",
      "\n",
      "Training a RandomForestClassifier using a training set size of 1117. . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\munis_000\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\munis_000\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score and accuracy score for training set: 0.9804 , 0.9830.\n",
      "F1 score and accuracy score for test set: 0.5863 , 0.6595.\n",
      "\n",
      "\n",
      "Training a SVC using a training set size of 1117. . .\n",
      "F1 score and accuracy score for training set: 0.6389 , 0.7046.\n",
      "F1 score and accuracy score for test set: 0.5267 , 0.6193.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Initialize the three models (XGBoost is initialized later)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "#import xgboost as xgb \n",
    "\n",
    "clf_A = LogisticRegression()\n",
    "clf_B = RandomForestClassifier()\n",
    "clf_C = SVC(random_state = 912, kernel='rbf',gamma='scale')\n",
    "#clf_D = xgb.XGBClassifier(seed = 82)\n",
    "\n",
    "train_predict(clf_A, X_train, y_train, X_test, y_test)\n",
    "\n",
    "train_predict(clf_B, X_train, y_train, X_test, y_test)\n",
    "\n",
    "train_predict(clf_C, X_train, y_train, X_test, y_test)\n",
    "\n",
    "#train_predict(clf_D, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hence Random forest is the best model as we can see form its f1 score and accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances = X_test.iloc[1].values.reshape(1, -1)\n",
    "clf_A.predict(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "duration_ms        -0.991440\n",
       "tempo              -1.232785\n",
       "energy             -0.522464\n",
       "danceability        0.082831\n",
       "acousticness       -0.422217\n",
       "instrumentalness   -0.132941\n",
       "liveness            3.252553\n",
       "loudness            1.688678\n",
       "popularity          1.228526\n",
       "speechiness        -0.492408\n",
       "valence            -0.388906\n",
       "Name: 745, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the parameters of Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "F1 score and accuracy score for training set: 100.0000% , 100.0000%\n",
      "F1 score and accuracy score for test set: 66.0377% , 71.0456%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "parameters = {'n_estimators':[100], \n",
    "              'criterion':[\"gini\"],    \n",
    "              'min_weight_fraction_leaf':[0],  \n",
    "              'min_impurity_decrease':[0],  \n",
    "              'verbose':[0]}\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "f1_scorer = make_scorer(f1_score,pos_label=1)\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(clf,\n",
    "                        scoring=f1_scorer,\n",
    "                        param_grid=parameters,\n",
    "                        cv=5)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train,y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "print (clf)\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "f1, acc = predict_labels(clf, X_train, y_train)\n",
    "print (\"F1 score and accuracy score for training set: {:.4f}% , {:.4f}%\".format((f1*100) , (acc*100)))\n",
    "    \n",
    "f1, acc = predict_labels(clf, X_test, y_test)\n",
    "print (\"F1 score and accuracy score for test set: {:.4f}% , {:.4f}%\".format((f1*100) , (acc*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
